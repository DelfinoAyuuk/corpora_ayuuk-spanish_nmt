2021-03-19 00:25:48,517 - INFO - root - Hello! This is Joey-NMT (version 1.0).
2021-03-19 00:25:48,640 - INFO - joeynmt.data - loading training data...
2021-03-19 00:25:49,068 - INFO - joeynmt.data - building vocabulary...
2021-03-19 00:25:49,280 - INFO - joeynmt.data - loading dev data...
2021-03-19 00:25:49,320 - INFO - joeynmt.data - loading test data...
2021-03-19 00:25:49,370 - INFO - joeynmt.data - data loaded.
2021-03-19 00:25:50,633 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-03-19 00:25:50,633 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-03-19 00:25:50,633 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-03-19 00:25:50,633 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-03-19 00:25:51,747 - INFO - joeynmt.training - Total params: 12098816
2021-03-19 00:25:51,749 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2021-03-19 00:25:51,751 - INFO - joeynmt.helpers - cfg.name                           : mir.esmir_transformer
2021-03-19 00:25:51,751 - INFO - joeynmt.helpers - cfg.data.src                       : es
2021-03-19 00:25:51,751 - INFO - joeynmt.helpers - cfg.data.trg                       : mir
2021-03-19 00:25:51,751 - INFO - joeynmt.helpers - cfg.data.train                     : /data/joeynmt/mir.esmir/train.bpe
2021-03-19 00:25:51,751 - INFO - joeynmt.helpers - cfg.data.dev                       : /data/joeynmt/mir.esmir/dev.bpe
2021-03-19 00:25:51,751 - INFO - joeynmt.helpers - cfg.data.test                      : /data/joeynmt/mir.esmir/test.bpe
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.data.lowercase                 : True
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 70
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /data/joeynmt/mir.esmir/vocab.txt
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /data/joeynmt/mir.esmir/vocab.txt
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 3
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.testing.postprocess            : True
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.testing.bpe_type               : subword-nmt
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.tokenize     : 13a
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.patience              : 5
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 800
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08
2021-03-19 00:25:51,752 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.batch_size            : 128
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.batch_type            : sentence
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 256
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : sentence
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.epochs                : 100
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 500
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/con-random5
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.overwrite             : False
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 70
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True
2021-03-19 00:25:51,753 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - Data set sizes: 
	train 5386,
	valid 700,
	test 912
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - First training example:
	[SRC] mas él, tom@@ ándo@@ la de la man@@ o, c@@ lam@@ ó diciendo much@@ a@@ ch@@ a, lev@@ án@@ ta@@ te
	[TRG] pero kyëꞌë gë@@ xpë jesus ja kixy uunk myaj@@ tsy, a nëm mëk nyëmaay kixy uunk@@ , ¡@@ pëd@@ ëꞌë@@ k@@ ! jantsy jujkpyë@@ jk ja kixy uunk@@ , mënityë pyë@@ dë@@ ꞌ@@ ky
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ja (5) ajxy (6) de (7) a (8) y (9) la
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ja (5) ajxy (6) de (7) a (8) y (9) la
2021-03-19 00:25:51,754 - INFO - joeynmt.helpers - Number of Src words (types): 4057
2021-03-19 00:25:51,755 - INFO - joeynmt.helpers - Number of Trg words (types): 4057
2021-03-19 00:25:51,755 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4057),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4057))
2021-03-19 00:25:51,758 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 128
	total batch size (w. parallel & accumulation): 128
2021-03-19 00:25:51,758 - INFO - joeynmt.training - EPOCH 1
2021-03-19 00:26:57,726 - INFO - joeynmt.training - Epoch   1: total training loss 265.61
2021-03-19 00:26:57,727 - INFO - joeynmt.training - EPOCH 2
2021-03-19 00:28:01,869 - INFO - joeynmt.training - Epoch   2: total training loss 246.38
2021-03-19 00:28:01,869 - INFO - joeynmt.training - EPOCH 3
2021-03-19 00:29:04,536 - INFO - joeynmt.training - Epoch   3: total training loss 244.00
2021-03-19 00:29:04,536 - INFO - joeynmt.training - EPOCH 4
2021-03-19 00:30:01,947 - INFO - joeynmt.training - Epoch   4: total training loss 241.83
2021-03-19 00:30:01,947 - INFO - joeynmt.training - EPOCH 5
2021-03-19 00:31:05,096 - INFO - joeynmt.training - Epoch   5: total training loss 239.22
2021-03-19 00:31:05,096 - INFO - joeynmt.training - EPOCH 6
2021-03-19 00:32:09,333 - INFO - joeynmt.training - Epoch   6: total training loss 237.36
2021-03-19 00:32:09,334 - INFO - joeynmt.training - EPOCH 7
2021-03-19 00:33:10,397 - INFO - joeynmt.training - Epoch   7: total training loss 234.49
2021-03-19 00:33:10,398 - INFO - joeynmt.training - EPOCH 8
2021-03-19 00:34:09,808 - INFO - joeynmt.training - Epoch   8: total training loss 231.80
2021-03-19 00:34:09,808 - INFO - joeynmt.training - EPOCH 9
2021-03-19 00:35:14,337 - INFO - joeynmt.training - Epoch   9: total training loss 228.09
2021-03-19 00:35:14,338 - INFO - joeynmt.training - EPOCH 10
2021-03-19 00:36:13,078 - INFO - joeynmt.training - Epoch  10: total training loss 224.24
2021-03-19 00:36:13,078 - INFO - joeynmt.training - EPOCH 11
2021-03-19 00:37:16,233 - INFO - joeynmt.training - Epoch  11: total training loss 219.99
2021-03-19 00:37:16,233 - INFO - joeynmt.training - EPOCH 12
2021-03-19 00:39:18,238 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2021-03-19 00:39:18,239 - INFO - joeynmt.training - Saving new checkpoint.
2021-03-19 00:39:18,590 - INFO - joeynmt.training - Example #0
2021-03-19 00:39:18,591 - DEBUG - joeynmt.training - 	Raw source:     ['promo@@', 'ver', 'el', 'uso', 'de', 'las', 'lengu@@', 'as']
2021-03-19 00:39:18,591 - DEBUG - joeynmt.training - 	Raw hypothesis: ["yë'ë", 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'juan', 'n@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', '?']
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Source:     promover el uso de las lenguas
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Reference:  yajtuyo'opy je ajxy jeky aambyë
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Hypothesis: yë'ë ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja ja juan nmmmmmmmmmm?
2021-03-19 00:39:18,591 - INFO - joeynmt.training - Example #1
2021-03-19 00:39:18,591 - DEBUG - joeynmt.training - 	Raw source:     ['pues', 'si', 'david', 'le', 'lla@@', 'ma', 'señor,', '¿cómo', 'es', 'su', 'hij@@', 'o?']
2021-03-19 00:39:18,591 - DEBUG - joeynmt.training - 	Raw hypothesis: ['a', 'a', 'a', 'a', 'ja', 'ja', 'ajxy', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të']
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Source:     pues si david le llama señor, ¿cómo es su hijo?
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Reference:  ¿nebyëk tyijyë david ja cristo jeꞌe duꞌun yꞌapꞌaty yꞌokꞌaty, pë david ja cristo jëduꞌun amdsoo yajxëꞌajp mëj windsën?
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Hypothesis: a a a a ja ja ajxy ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të
2021-03-19 00:39:18,591 - INFO - joeynmt.training - Example #2
2021-03-19 00:39:18,591 - DEBUG - joeynmt.training - 	Raw source:     ['¿cuán@@', 'do', 'fue', 'autoridad', 'ju@@', 'an@@', '?']
2021-03-19 00:39:18,591 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¿@@', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'ajxy', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të']
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Source:     ¿cuándo fue autoridad juan?
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Reference:  ¿mënaa ja juan tëë tyunkmëëdaty?
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Hypothesis: ¿ja ja ja ja ja ja ja ja ja ja ajxy të të të të të të të të të të të të të të të të
2021-03-19 00:39:18,591 - INFO - joeynmt.training - Example #3
2021-03-19 00:39:18,591 - DEBUG - joeynmt.training - 	Raw source:     ['porque', 'cualquiera', 'que', 'se', 'en@@', 'al@@', 'tec@@', 'e,', 'será', 'hu@@', 'mi@@', 'll@@', 'ado', 'y', 'el', 'que', 'se', 'hu@@', 'mi@@', 'll@@', 'a,', 'será', 'en@@', 'al@@', 'te@@', 'ci@@', 'do']
2021-03-19 00:39:18,591 - DEBUG - joeynmt.training - 	Raw hypothesis: ['ku', 'ja', 'ja', 'ja', 'ja', 'ja', 'ajxy', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty', 'ënajty']
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Source:     porque cualquiera que se enaltece, será humillado y el que se humilla, será enaltecido
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Reference:  ix pën jaty nëmyëjjaantsypëdaakëp, tsyuuk pëdaꞌaganëp jeꞌe a pën nëtsyuukpëdaakëp, jeꞌejëts mëj jaantsy pëdaꞌaganëp
2021-03-19 00:39:18,591 - INFO - joeynmt.training - 	Hypothesis: ku ja ja ja ja ja ajxy ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty ënajty
2021-03-19 00:39:18,592 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step      500: bleu:   0.06, loss: 118366.0938, ppl: 167.4606, duration: 81.3536s
2021-03-19 00:39:42,287 - INFO - joeynmt.training - Epoch  12: total training loss 214.71
2021-03-19 00:39:42,287 - INFO - joeynmt.training - EPOCH 13
2021-03-19 00:40:39,675 - INFO - joeynmt.training - Epoch  13: total training loss 210.41
2021-03-19 00:40:39,676 - INFO - joeynmt.training - EPOCH 14
2021-03-19 00:41:44,790 - INFO - joeynmt.training - Epoch  14: total training loss 207.62
2021-03-19 00:41:44,791 - INFO - joeynmt.training - EPOCH 15
2021-03-19 00:42:50,429 - INFO - joeynmt.training - Epoch  15: total training loss 204.60
2021-03-19 00:42:50,429 - INFO - joeynmt.training - EPOCH 16
2021-03-19 00:43:48,113 - INFO - joeynmt.training - Epoch  16: total training loss 201.51
2021-03-19 00:43:48,114 - INFO - joeynmt.training - EPOCH 17
2021-03-19 00:45:25,639 - INFO - joeynmt.training - Epoch  17: total training loss 199.24
2021-03-19 00:45:25,641 - INFO - joeynmt.training - EPOCH 18
2021-03-19 00:47:05,476 - INFO - joeynmt.training - Epoch  18: total training loss 196.75
2021-03-19 00:47:05,477 - INFO - joeynmt.training - EPOCH 19
2021-03-19 00:48:09,550 - INFO - joeynmt.training - Epoch  19: total training loss 194.77
2021-03-19 00:48:09,552 - INFO - joeynmt.training - EPOCH 20
2021-03-19 00:49:14,379 - INFO - joeynmt.training - Epoch  20: total training loss 192.28
2021-03-19 00:49:14,380 - INFO - joeynmt.training - EPOCH 21
2021-03-19 00:50:16,481 - INFO - joeynmt.training - Epoch  21: total training loss 189.93
2021-03-19 00:50:16,481 - INFO - joeynmt.training - EPOCH 22
2021-03-19 00:51:20,550 - INFO - joeynmt.training - Epoch  22: total training loss 187.86
2021-03-19 00:51:20,551 - INFO - joeynmt.training - EPOCH 23
2021-03-19 00:52:26,834 - INFO - joeynmt.training - Epoch  23: total training loss 186.17
2021-03-19 00:52:26,835 - INFO - joeynmt.training - EPOCH 24
2021-03-19 00:52:42,838 - INFO - joeynmt.training - Epoch  24, Step:     1000, Batch Loss:     4.302063, Tokens per Sec:     2185, Lr: 0.000300
2021-03-19 00:54:09,822 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2021-03-19 00:54:09,822 - INFO - joeynmt.training - Saving new checkpoint.
2021-03-19 00:54:10,083 - INFO - joeynmt.training - Example #0
2021-03-19 00:54:10,083 - DEBUG - joeynmt.training - 	Raw source:     ['promo@@', 'ver', 'el', 'uso', 'de', 'las', 'lengu@@', 'as']
2021-03-19 00:54:10,083 - DEBUG - joeynmt.training - 	Raw hypothesis: ["yë'ë", 'kixy', "yë'ë", 'ajxy', 'tëë', 'tëë']
2021-03-19 00:54:10,083 - INFO - joeynmt.training - 	Source:     promover el uso de las lenguas
2021-03-19 00:54:10,083 - INFO - joeynmt.training - 	Reference:  yajtuyo'opy je ajxy jeky aambyë
2021-03-19 00:54:10,083 - INFO - joeynmt.training - 	Hypothesis: yë'ë kixy yë'ë ajxy tëë tëë
2021-03-19 00:54:10,083 - INFO - joeynmt.training - Example #1
2021-03-19 00:54:10,083 - DEBUG - joeynmt.training - 	Raw source:     ['pues', 'si', 'david', 'le', 'lla@@', 'ma', 'señor,', '¿cómo', 'es', 'su', 'hij@@', 'o?']
2021-03-19 00:54:10,083 - DEBUG - joeynmt.training - 	Raw hypothesis: ['nëmë', 'jesus', 'yꞌadsooy', '¿@@', 'jëduꞌun', 'jëduꞌun', 'jëduꞌun', 'jëduꞌun', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të']
2021-03-19 00:54:10,083 - INFO - joeynmt.training - 	Source:     pues si david le llama señor, ¿cómo es su hijo?
2021-03-19 00:54:10,084 - INFO - joeynmt.training - 	Reference:  ¿nebyëk tyijyë david ja cristo jeꞌe duꞌun yꞌapꞌaty yꞌokꞌaty, pë david ja cristo jëduꞌun amdsoo yajxëꞌajp mëj windsën?
2021-03-19 00:54:10,084 - INFO - joeynmt.training - 	Hypothesis: nëmë jesus yꞌadsooy ¿jëduꞌun jëduꞌun jëduꞌun jëduꞌun të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të
2021-03-19 00:54:10,084 - INFO - joeynmt.training - Example #2
2021-03-19 00:54:10,084 - DEBUG - joeynmt.training - 	Raw source:     ['¿cuán@@', 'do', 'fue', 'autoridad', 'ju@@', 'an@@', '?']
2021-03-19 00:54:10,084 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¿@@', '¿@@', '¿@@', '¿@@', '?']
2021-03-19 00:54:10,084 - INFO - joeynmt.training - 	Source:     ¿cuándo fue autoridad juan?
2021-03-19 00:54:10,084 - INFO - joeynmt.training - 	Reference:  ¿mënaa ja juan tëë tyunkmëëdaty?
2021-03-19 00:54:10,084 - INFO - joeynmt.training - 	Hypothesis: ¿¿¿¿?
2021-03-19 00:54:10,084 - INFO - joeynmt.training - Example #3
2021-03-19 00:54:10,084 - DEBUG - joeynmt.training - 	Raw source:     ['porque', 'cualquiera', 'que', 'se', 'en@@', 'al@@', 'tec@@', 'e,', 'será', 'hu@@', 'mi@@', 'll@@', 'ado', 'y', 'el', 'que', 'se', 'hu@@', 'mi@@', 'll@@', 'a,', 'será', 'en@@', 'al@@', 'te@@', 'ci@@', 'do']
2021-03-19 00:54:10,084 - DEBUG - joeynmt.training - 	Raw hypothesis: ['a', 'ku', 'ja', 'yaꞌay', 'ajxy', 'ënajty', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të', 'të']
2021-03-19 00:54:10,084 - INFO - joeynmt.training - 	Source:     porque cualquiera que se enaltece, será humillado y el que se humilla, será enaltecido
2021-03-19 00:54:10,084 - INFO - joeynmt.training - 	Reference:  ix pën jaty nëmyëjjaantsypëdaakëp, tsyuuk pëdaꞌaganëp jeꞌe a pën nëtsyuukpëdaakëp, jeꞌejëts mëj jaantsy pëdaꞌaganëp
2021-03-19 00:54:10,084 - INFO - joeynmt.training - 	Hypothesis: a ku ja yaꞌay ajxy ënajty të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të të
2021-03-19 00:54:10,084 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step     1000: bleu:   0.38, loss: 105258.1172, ppl:  94.9805, duration: 87.2451s
2021-03-19 00:54:58,597 - INFO - joeynmt.training - Epoch  24: total training loss 183.43
2021-03-19 00:54:58,597 - INFO - joeynmt.training - EPOCH 25
2021-03-19 00:56:04,654 - INFO - joeynmt.training - Epoch  25: total training loss 181.02
2021-03-19 00:56:04,655 - INFO - joeynmt.training - EPOCH 26
2021-03-19 00:57:10,432 - INFO - joeynmt.training - Epoch  26: total training loss 179.50
2021-03-19 00:57:10,432 - INFO - joeynmt.training - EPOCH 27
2021-03-19 00:58:17,802 - INFO - joeynmt.training - Epoch  27: total training loss 176.52
2021-03-19 00:58:17,803 - INFO - joeynmt.training - EPOCH 28
2021-03-19 00:59:18,672 - INFO - joeynmt.training - Epoch  28: total training loss 174.90
2021-03-19 00:59:18,673 - INFO - joeynmt.training - EPOCH 29
2021-03-19 01:00:24,403 - INFO - joeynmt.training - Epoch  29: total training loss 172.15
2021-03-19 01:00:24,404 - INFO - joeynmt.training - EPOCH 30
2021-03-19 01:01:29,479 - INFO - joeynmt.training - Epoch  30: total training loss 170.57
2021-03-19 01:01:29,479 - INFO - joeynmt.training - EPOCH 31
2021-03-19 01:02:33,315 - INFO - joeynmt.training - Epoch  31: total training loss 167.79
2021-03-19 01:02:33,316 - INFO - joeynmt.training - EPOCH 32
2021-03-19 01:03:36,256 - INFO - joeynmt.training - Epoch  32: total training loss 165.67
2021-03-19 01:03:36,256 - INFO - joeynmt.training - EPOCH 33
2021-03-19 01:04:39,923 - INFO - joeynmt.training - Epoch  33: total training loss 163.17
2021-03-19 01:04:39,923 - INFO - joeynmt.training - EPOCH 34
2021-03-19 01:05:42,001 - INFO - joeynmt.training - Epoch  34: total training loss 160.80
2021-03-19 01:05:42,002 - INFO - joeynmt.training - EPOCH 35
2021-03-19 01:08:17,495 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2021-03-19 01:08:17,495 - INFO - joeynmt.training - Saving new checkpoint.
2021-03-19 01:08:17,799 - INFO - joeynmt.training - Example #0
2021-03-19 01:08:17,799 - DEBUG - joeynmt.training - 	Raw source:     ['promo@@', 'ver', 'el', 'uso', 'de', 'las', 'lengu@@', 'as']
2021-03-19 01:08:17,799 - DEBUG - joeynmt.training - 	Raw hypothesis: ["yë'ë", 'ajxy', "je'e", "du'un", 'oy', 'oy', 'oy', 'oy', 'oy', 'oy', 'oyë', 'oy', 'oy', 'oy', 'oy', 'oyë', 'të', 'nyëk@@', 'ë']
2021-03-19 01:08:17,799 - INFO - joeynmt.training - 	Source:     promover el uso de las lenguas
2021-03-19 01:08:17,799 - INFO - joeynmt.training - 	Reference:  yajtuyo'opy je ajxy jeky aambyë
2021-03-19 01:08:17,799 - INFO - joeynmt.training - 	Hypothesis: yë'ë ajxy je'e du'un oy oy oy oy oy oy oyë oy oy oy oy oyë të nyëkë
2021-03-19 01:08:17,799 - INFO - joeynmt.training - Example #1
2021-03-19 01:08:17,799 - DEBUG - joeynmt.training - 	Raw source:     ['pues', 'si', 'david', 'le', 'lla@@', 'ma', 'señor,', '¿cómo', 'es', 'su', 'hij@@', 'o?']
2021-03-19 01:08:17,800 - DEBUG - joeynmt.training - 	Raw hypothesis: ['a', 'paadyëts,', 'paadyëts,', 'paadyëts,', 'paadyëts,', 'paadyëts,', 'paadyëts,', 'paadyëts,', 'paadyëts,', 'paadyëts,', 'kap', 'jeꞌe', 'jeꞌe', 'duꞌun', 'ku', 'ja', 'diosë', 'myëdyaꞌaky', 'jëgapxnajx@@', 'pëdëjk', 'ajxy', 'jëduꞌun', 'ku', 'ja', 'diosë', 'myëdyaꞌaky', 'jëgapxnajx@@', 'pëdëjk', 'ajxy', 'jëduꞌun', 'të', 'xykye@@', 'xy']
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Source:     pues si david le llama señor, ¿cómo es su hijo?
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Reference:  ¿nebyëk tyijyë david ja cristo jeꞌe duꞌun yꞌapꞌaty yꞌokꞌaty, pë david ja cristo jëduꞌun amdsoo yajxëꞌajp mëj windsën?
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Hypothesis: a paadyëts, paadyëts, paadyëts, paadyëts, paadyëts, paadyëts, paadyëts, paadyëts, paadyëts, kap jeꞌe jeꞌe duꞌun ku ja diosë myëdyaꞌaky jëgapxnajxpëdëjk ajxy jëduꞌun ku ja diosë myëdyaꞌaky jëgapxnajxpëdëjk ajxy jëduꞌun të xykyexy
2021-03-19 01:08:17,800 - INFO - joeynmt.training - Example #2
2021-03-19 01:08:17,800 - DEBUG - joeynmt.training - 	Raw source:     ['¿cuán@@', 'do', 'fue', 'autoridad', 'ju@@', 'an@@', '?']
2021-03-19 01:08:17,800 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¿@@', 'maa', 'ja', 'juan', 'm@@', 'ju@@', 'yë@@', '?']
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Source:     ¿cuándo fue autoridad juan?
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Reference:  ¿mënaa ja juan tëë tyunkmëëdaty?
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Hypothesis: ¿maa ja juan mjuyë?
2021-03-19 01:08:17,800 - INFO - joeynmt.training - Example #3
2021-03-19 01:08:17,800 - DEBUG - joeynmt.training - 	Raw source:     ['porque', 'cualquiera', 'que', 'se', 'en@@', 'al@@', 'tec@@', 'e,', 'será', 'hu@@', 'mi@@', 'll@@', 'ado', 'y', 'el', 'que', 'se', 'hu@@', 'mi@@', 'll@@', 'a,', 'será', 'en@@', 'al@@', 'te@@', 'ci@@', 'do']
2021-03-19 01:08:17,800 - DEBUG - joeynmt.training - 	Raw hypothesis: ['ix', 'pën', 'jeꞌe', 'duꞌun', 'të', 'kyaꞌa@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'aꞌa@@', 'ky,', 'a', 'jëduꞌun', 'jeꞌe', 'jeꞌe', 'jeꞌe', 'jeꞌe', 'jeꞌe', 'jeꞌe', 'gyëxpë', 'ku', 'jeꞌe', 'duꞌun', 'të', 'kyaꞌa@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'ꞌ@@', 'id@@', 'ët']
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Source:     porque cualquiera que se enaltece, será humillado y el que se humilla, será enaltecido
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Reference:  ix pën jaty nëmyëjjaantsypëdaakëp, tsyuuk pëdaꞌaganëp jeꞌe a pën nëtsyuukpëdaakëp, jeꞌejëts mëj jaantsy pëdaꞌaganëp
2021-03-19 01:08:17,800 - INFO - joeynmt.training - 	Hypothesis: ix pën jeꞌe duꞌun të kyaꞌaꞌꞌꞌꞌꞌꞌꞌꞌꞌꞌꞌꞌꞌꞌaꞌaky, a jëduꞌun jeꞌe jeꞌe jeꞌe jeꞌe jeꞌe jeꞌe gyëxpë ku jeꞌe duꞌun të kyaꞌaꞌꞌꞌꞌidët
2021-03-19 01:08:17,800 - INFO - joeynmt.training - Validation result (greedy) at epoch  35, step     1500: bleu:   2.35, loss: 96310.5000, ppl:  64.4945, duration: 92.1062s
2021-03-19 01:08:25,261 - INFO - joeynmt.training - Epoch  35: total training loss 159.00
2021-03-19 01:08:25,261 - INFO - joeynmt.training - EPOCH 36
2021-03-19 01:09:31,820 - INFO - joeynmt.training - Epoch  36: total training loss 156.25
2021-03-19 01:09:31,820 - INFO - joeynmt.training - EPOCH 37
2021-03-19 01:10:39,375 - INFO - joeynmt.training - Epoch  37: total training loss 154.18
2021-03-19 01:10:39,375 - INFO - joeynmt.training - EPOCH 38
2021-03-19 01:11:47,533 - INFO - joeynmt.training - Epoch  38: total training loss 152.31
2021-03-19 01:11:47,533 - INFO - joeynmt.training - EPOCH 39
2021-03-19 01:12:56,471 - INFO - joeynmt.training - Epoch  39: total training loss 150.72
2021-03-19 01:12:56,471 - INFO - joeynmt.training - EPOCH 40
2021-03-19 01:14:03,110 - INFO - joeynmt.training - Epoch  40: total training loss 149.41
2021-03-19 01:14:03,111 - INFO - joeynmt.training - EPOCH 41
2021-03-19 01:15:13,827 - INFO - joeynmt.training - Epoch  41: total training loss 146.68
2021-03-19 01:15:13,829 - INFO - joeynmt.training - EPOCH 42
2021-03-19 01:16:14,295 - INFO - joeynmt.training - Epoch  42: total training loss 144.42
2021-03-19 01:16:14,296 - INFO - joeynmt.training - EPOCH 43
2021-03-19 01:17:20,436 - INFO - joeynmt.training - Epoch  43: total training loss 142.69
2021-03-19 01:17:20,438 - INFO - joeynmt.training - EPOCH 44
2021-03-19 01:18:27,455 - INFO - joeynmt.training - Epoch  44: total training loss 140.87
2021-03-19 01:18:27,456 - INFO - joeynmt.training - EPOCH 45
2021-03-19 01:19:32,811 - INFO - joeynmt.training - Epoch  45: total training loss 139.42
2021-03-19 01:19:32,811 - INFO - joeynmt.training - EPOCH 46
2021-03-19 01:20:33,467 - INFO - joeynmt.training - Epoch  46: total training loss 137.28
2021-03-19 01:20:33,468 - INFO - joeynmt.training - EPOCH 47
2021-03-19 01:21:06,699 - INFO - joeynmt.training - Epoch  47, Step:     2000, Batch Loss:     2.821314, Tokens per Sec:     2026, Lr: 0.000300
2021-03-19 01:22:38,054 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2021-03-19 01:22:38,055 - INFO - joeynmt.training - Saving new checkpoint.
2021-03-19 01:22:38,345 - INFO - joeynmt.training - Example #0
2021-03-19 01:22:38,345 - DEBUG - joeynmt.training - 	Raw source:     ['promo@@', 'ver', 'el', 'uso', 'de', 'las', 'lengu@@', 'as']
2021-03-19 01:22:38,345 - DEBUG - joeynmt.training - 	Raw hypothesis: ["yë'ë", 'ajxy', 'ayuuk', "mëdya'aky", 'myëëdë']
2021-03-19 01:22:38,345 - INFO - joeynmt.training - 	Source:     promover el uso de las lenguas
2021-03-19 01:22:38,345 - INFO - joeynmt.training - 	Reference:  yajtuyo'opy je ajxy jeky aambyë
2021-03-19 01:22:38,345 - INFO - joeynmt.training - 	Hypothesis: yë'ë ajxy ayuuk mëdya'aky myëëdë
2021-03-19 01:22:38,345 - INFO - joeynmt.training - Example #1
2021-03-19 01:22:38,345 - DEBUG - joeynmt.training - 	Raw source:     ['pues', 'si', 'david', 'le', 'lla@@', 'ma', 'señor,', '¿cómo', 'es', 'su', 'hij@@', 'o?']
2021-03-19 01:22:38,345 - DEBUG - joeynmt.training - 	Raw hypothesis: ['pero', 'paadyëts,', 'jëduꞌun', 'ku', 'ja', 'diosë', 'myëdyaꞌaky', 'jëgapxnajx@@', 'pëdëjk', 'ajxy', 'ënajty', 'myënaꞌany', '¿@@', 'mëdyii', 'yëꞌë', 'duꞌun', 'të', 'm@@', 'ꞌix@@', 'y?']
2021-03-19 01:22:38,345 - INFO - joeynmt.training - 	Source:     pues si david le llama señor, ¿cómo es su hijo?
2021-03-19 01:22:38,345 - INFO - joeynmt.training - 	Reference:  ¿nebyëk tyijyë david ja cristo jeꞌe duꞌun yꞌapꞌaty yꞌokꞌaty, pë david ja cristo jëduꞌun amdsoo yajxëꞌajp mëj windsën?
2021-03-19 01:22:38,345 - INFO - joeynmt.training - 	Hypothesis: pero paadyëts, jëduꞌun ku ja diosë myëdyaꞌaky jëgapxnajxpëdëjk ajxy ënajty myënaꞌany ¿mëdyii yëꞌë duꞌun të mꞌixy?
2021-03-19 01:22:38,345 - INFO - joeynmt.training - Example #2
2021-03-19 01:22:38,346 - DEBUG - joeynmt.training - 	Raw source:     ['¿cuán@@', 'do', 'fue', 'autoridad', 'ju@@', 'an@@', '?']
2021-03-19 01:22:38,346 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¿@@', 'maa', 'ja', 'yaꞌay', 'ajxy', 'jëduꞌun', 'të', 'n@@', 'ꞌij@@', 'xëꞌë@@', 'ky?']
2021-03-19 01:22:38,346 - INFO - joeynmt.training - 	Source:     ¿cuándo fue autoridad juan?
2021-03-19 01:22:38,346 - INFO - joeynmt.training - 	Reference:  ¿mënaa ja juan tëë tyunkmëëdaty?
2021-03-19 01:22:38,346 - INFO - joeynmt.training - 	Hypothesis: ¿maa ja yaꞌay ajxy jëduꞌun të nꞌijxëꞌëky?
2021-03-19 01:22:38,346 - INFO - joeynmt.training - Example #3
2021-03-19 01:22:38,346 - DEBUG - joeynmt.training - 	Raw source:     ['porque', 'cualquiera', 'que', 'se', 'en@@', 'al@@', 'tec@@', 'e,', 'será', 'hu@@', 'mi@@', 'll@@', 'ado', 'y', 'el', 'que', 'se', 'hu@@', 'mi@@', 'll@@', 'a,', 'será', 'en@@', 'al@@', 'te@@', 'ci@@', 'do']
2021-03-19 01:22:38,346 - DEBUG - joeynmt.training - 	Raw hypothesis: ['ix', 'pën', 'jeꞌe', 'duꞌun', 'të', 'xykye@@', 'xy', 'ma', 'ja', 'yaabë', 'naax', 'wiimbë', 'jëyaꞌay', 'ajxyën']
2021-03-19 01:22:38,346 - INFO - joeynmt.training - 	Source:     porque cualquiera que se enaltece, será humillado y el que se humilla, será enaltecido
2021-03-19 01:22:38,346 - INFO - joeynmt.training - 	Reference:  ix pën jaty nëmyëjjaantsypëdaakëp, tsyuuk pëdaꞌaganëp jeꞌe a pën nëtsyuukpëdaakëp, jeꞌejëts mëj jaantsy pëdaꞌaganëp
2021-03-19 01:22:38,346 - INFO - joeynmt.training - 	Hypothesis: ix pën jeꞌe duꞌun të xykyexy ma ja yaabë naax wiimbë jëyaꞌay ajxyën
2021-03-19 01:22:38,346 - INFO - joeynmt.training - Validation result (greedy) at epoch  47, step     2000: bleu:   3.70, loss: 91623.2500, ppl:  52.6571, duration: 91.6464s
2021-03-19 01:23:12,013 - INFO - joeynmt.training - Epoch  47: total training loss 136.17
2021-03-19 01:23:12,013 - INFO - joeynmt.training - EPOCH 48
2021-03-19 01:24:17,290 - INFO - joeynmt.training - Epoch  48: total training loss 134.48
2021-03-19 01:24:17,290 - INFO - joeynmt.training - EPOCH 49
2021-03-19 01:25:17,155 - INFO - joeynmt.training - Epoch  49: total training loss 133.05
2021-03-19 01:25:17,155 - INFO - joeynmt.training - EPOCH 50
2021-03-19 01:26:22,650 - INFO - joeynmt.training - Epoch  50: total training loss 130.92
2021-03-19 01:26:22,651 - INFO - joeynmt.training - EPOCH 51
2021-03-19 01:27:30,550 - INFO - joeynmt.training - Epoch  51: total training loss 129.20
2021-03-19 01:27:30,550 - INFO - joeynmt.training - EPOCH 52
2021-03-19 01:28:37,420 - INFO - joeynmt.training - Epoch  52: total training loss 127.84
2021-03-19 01:28:37,421 - INFO - joeynmt.training - EPOCH 53
2021-03-19 01:29:42,905 - INFO - joeynmt.training - Epoch  53: total training loss 126.03
2021-03-19 01:29:42,905 - INFO - joeynmt.training - EPOCH 54
2021-03-19 01:30:46,404 - INFO - joeynmt.training - Epoch  54: total training loss 125.24
2021-03-19 01:30:46,405 - INFO - joeynmt.training - EPOCH 55
2021-03-19 01:31:55,192 - INFO - joeynmt.training - Epoch  55: total training loss 123.22
2021-03-19 01:31:55,192 - INFO - joeynmt.training - EPOCH 56
2021-03-19 01:33:01,596 - INFO - joeynmt.training - Epoch  56: total training loss 121.77
2021-03-19 01:33:01,597 - INFO - joeynmt.training - EPOCH 57
2021-03-19 01:34:06,670 - INFO - joeynmt.training - Epoch  57: total training loss 120.16
2021-03-19 01:34:06,671 - INFO - joeynmt.training - EPOCH 58
2021-03-19 01:35:08,922 - INFO - joeynmt.training - Epoch  58: total training loss 118.80
2021-03-19 01:35:08,922 - INFO - joeynmt.training - EPOCH 59
2021-03-19 01:36:52,443 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2021-03-19 01:36:52,444 - INFO - joeynmt.training - Saving new checkpoint.
2021-03-19 01:36:52,744 - INFO - joeynmt.training - Example #0
2021-03-19 01:36:52,744 - DEBUG - joeynmt.training - 	Raw source:     ['promo@@', 'ver', 'el', 'uso', 'de', 'las', 'lengu@@', 'as']
2021-03-19 01:36:52,744 - DEBUG - joeynmt.training - 	Raw hypothesis: ['majk', 'pe@@', 'kyë@@', "na'a@@", 'k', 'adaa']
2021-03-19 01:36:52,744 - INFO - joeynmt.training - 	Source:     promover el uso de las lenguas
2021-03-19 01:36:52,744 - INFO - joeynmt.training - 	Reference:  yajtuyo'opy je ajxy jeky aambyë
2021-03-19 01:36:52,744 - INFO - joeynmt.training - 	Hypothesis: majk pekyëna'ak adaa
2021-03-19 01:36:52,744 - INFO - joeynmt.training - Example #1
2021-03-19 01:36:52,744 - DEBUG - joeynmt.training - 	Raw source:     ['pues', 'si', 'david', 'le', 'lla@@', 'ma', 'señor,', '¿cómo', 'es', 'su', 'hij@@', 'o?']
2021-03-19 01:36:52,744 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¡@@', 'maas', 'tsobaa@@', 'tpë', 'jëyaꞌay', 'yëꞌë', 'duꞌun@@', ',', 'a', 'maas', 'tsobaa@@', 'tpë', 'jëyaꞌay', 'ajxy', 'jeꞌe', 'duꞌun', 'ku', 'ja', 'diosë', 'myëdyaꞌaky', 'jëgapxnajxpë', 'ja', 'yaꞌay', 'ajxy', 'jëduꞌun', 'të', 'm@@', 'mëdyaꞌa@@', 'ky?']
2021-03-19 01:36:52,744 - INFO - joeynmt.training - 	Source:     pues si david le llama señor, ¿cómo es su hijo?
2021-03-19 01:36:52,744 - INFO - joeynmt.training - 	Reference:  ¿nebyëk tyijyë david ja cristo jeꞌe duꞌun yꞌapꞌaty yꞌokꞌaty, pë david ja cristo jëduꞌun amdsoo yajxëꞌajp mëj windsën?
2021-03-19 01:36:52,744 - INFO - joeynmt.training - 	Hypothesis: ¡maas tsobaatpë jëyaꞌay yëꞌë duꞌun, a maas tsobaatpë jëyaꞌay ajxy jeꞌe duꞌun ku ja diosë myëdyaꞌaky jëgapxnajxpë ja yaꞌay ajxy jëduꞌun të mmëdyaꞌaky?
2021-03-19 01:36:52,744 - INFO - joeynmt.training - Example #2
2021-03-19 01:36:52,744 - DEBUG - joeynmt.training - 	Raw source:     ['¿cuán@@', 'do', 'fue', 'autoridad', 'ju@@', 'an@@', '?']
2021-03-19 01:36:52,744 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¿@@', 'mënaa', 'ja', 'juan', 'tëë', 'xy@@', 'ty@@', 'ty@@', '?']
2021-03-19 01:36:52,745 - INFO - joeynmt.training - 	Source:     ¿cuándo fue autoridad juan?
2021-03-19 01:36:52,745 - INFO - joeynmt.training - 	Reference:  ¿mënaa ja juan tëë tyunkmëëdaty?
2021-03-19 01:36:52,745 - INFO - joeynmt.training - 	Hypothesis: ¿mënaa ja juan tëë xytyty?
2021-03-19 01:36:52,745 - INFO - joeynmt.training - Example #3
2021-03-19 01:36:52,745 - DEBUG - joeynmt.training - 	Raw source:     ['porque', 'cualquiera', 'que', 'se', 'en@@', 'al@@', 'tec@@', 'e,', 'será', 'hu@@', 'mi@@', 'll@@', 'ado', 'y', 'el', 'que', 'se', 'hu@@', 'mi@@', 'll@@', 'a,', 'será', 'en@@', 'al@@', 'te@@', 'ci@@', 'do']
2021-03-19 01:36:52,745 - DEBUG - joeynmt.training - 	Raw hypothesis: ['ix', 'pën', 'jeꞌe', 'duꞌun', 'të', 'm@@', 'mëdo@@', 'y,', 'ix', 'pën', 'jeꞌe', 'duꞌun', 'myë@@', 'yujꞌw@@', 'aampy', 'nebyë', 'dios', 'tsyo@@', 'kyën,', 'ix', 'ja', 'juꞌukyꞌajt', 'këjxtaꞌa@@', 'xyëëbë', 'jeꞌe', 'duꞌun', 'të', 'tyun@@', 'yëëjën']
2021-03-19 01:36:52,745 - INFO - joeynmt.training - 	Source:     porque cualquiera que se enaltece, será humillado y el que se humilla, será enaltecido
2021-03-19 01:36:52,745 - INFO - joeynmt.training - 	Reference:  ix pën jaty nëmyëjjaantsypëdaakëp, tsyuuk pëdaꞌaganëp jeꞌe a pën nëtsyuukpëdaakëp, jeꞌejëts mëj jaantsy pëdaꞌaganëp
2021-03-19 01:36:52,745 - INFO - joeynmt.training - 	Hypothesis: ix pën jeꞌe duꞌun të mmëdoy, ix pën jeꞌe duꞌun myëyujꞌwaampy nebyë dios tsyokyën, ix ja juꞌukyꞌajt këjxtaꞌaxyëëbë jeꞌe duꞌun të tyunyëëjën
2021-03-19 01:36:52,745 - INFO - joeynmt.training - Validation result (greedy) at epoch  59, step     2500: bleu:   4.95, loss: 88420.9375, ppl:  45.8448, duration: 93.5417s
2021-03-19 01:37:48,751 - INFO - joeynmt.training - Epoch  59: total training loss 117.23
2021-03-19 01:37:48,751 - INFO - joeynmt.training - EPOCH 60
2021-03-19 01:38:55,217 - INFO - joeynmt.training - Epoch  60: total training loss 116.05
2021-03-19 01:38:55,217 - INFO - joeynmt.training - EPOCH 61
2021-03-19 01:39:59,931 - INFO - joeynmt.training - Epoch  61: total training loss 115.06
2021-03-19 01:39:59,931 - INFO - joeynmt.training - EPOCH 62
2021-03-19 01:41:02,249 - INFO - joeynmt.training - Epoch  62: total training loss 114.37
2021-03-19 01:41:02,250 - INFO - joeynmt.training - EPOCH 63
2021-03-19 01:42:09,801 - INFO - joeynmt.training - Epoch  63: total training loss 112.88
2021-03-19 01:42:09,801 - INFO - joeynmt.training - EPOCH 64
2021-03-19 01:43:18,777 - INFO - joeynmt.training - Epoch  64: total training loss 110.85
2021-03-19 01:43:18,777 - INFO - joeynmt.training - EPOCH 65
2021-03-19 01:44:26,047 - INFO - joeynmt.training - Epoch  65: total training loss 109.71
2021-03-19 01:44:26,047 - INFO - joeynmt.training - EPOCH 66
2021-03-19 01:45:34,467 - INFO - joeynmt.training - Epoch  66: total training loss 108.50
2021-03-19 01:45:34,468 - INFO - joeynmt.training - EPOCH 67
2021-03-19 01:46:43,641 - INFO - joeynmt.training - Epoch  67: total training loss 107.29
2021-03-19 01:46:43,641 - INFO - joeynmt.training - EPOCH 68
2021-03-19 01:47:47,257 - INFO - joeynmt.training - Epoch  68: total training loss 106.13
2021-03-19 01:47:47,258 - INFO - joeynmt.training - EPOCH 69
2021-03-19 01:48:56,562 - INFO - joeynmt.training - Epoch  69: total training loss 105.08
2021-03-19 01:48:56,563 - INFO - joeynmt.training - EPOCH 70
2021-03-19 01:49:43,924 - INFO - joeynmt.training - Epoch  70, Step:     3000, Batch Loss:     2.838881, Tokens per Sec:     2230, Lr: 0.000300
2021-03-19 01:51:19,577 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2021-03-19 01:51:19,578 - INFO - joeynmt.training - Saving new checkpoint.
2021-03-19 01:51:20,014 - INFO - joeynmt.training - Example #0
2021-03-19 01:51:20,014 - DEBUG - joeynmt.training - 	Raw source:     ['promo@@', 'ver', 'el', 'uso', 'de', 'las', 'lengu@@', 'as']
2021-03-19 01:51:20,014 - DEBUG - joeynmt.training - 	Raw hypothesis: ['majk', 'pe@@', 'kyë@@', 'xë@@', 'pë']
2021-03-19 01:51:20,014 - INFO - joeynmt.training - 	Source:     promover el uso de las lenguas
2021-03-19 01:51:20,014 - INFO - joeynmt.training - 	Reference:  yajtuyo'opy je ajxy jeky aambyë
2021-03-19 01:51:20,014 - INFO - joeynmt.training - 	Hypothesis: majk pekyëxëpë
2021-03-19 01:51:20,015 - INFO - joeynmt.training - Example #1
2021-03-19 01:51:20,015 - DEBUG - joeynmt.training - 	Raw source:     ['pues', 'si', 'david', 'le', 'lla@@', 'ma', 'señor,', '¿cómo', 'es', 'su', 'hij@@', 'o?']
2021-03-19 01:51:20,015 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¡@@', 'jëduꞌun', 'mëwiin', 'nebyë', 'dios', 'teety', 'të', 'myënaꞌan@@', 'yën,', 'jëduꞌun', 'ajxy', 'jeꞌe', 'këxyë', 'wiinë', 'wiinë', 'wiinë', 'a', 'maas', 'tsobaa@@', 't@@', '!']
2021-03-19 01:51:20,015 - INFO - joeynmt.training - 	Source:     pues si david le llama señor, ¿cómo es su hijo?
2021-03-19 01:51:20,015 - INFO - joeynmt.training - 	Reference:  ¿nebyëk tyijyë david ja cristo jeꞌe duꞌun yꞌapꞌaty yꞌokꞌaty, pë david ja cristo jëduꞌun amdsoo yajxëꞌajp mëj windsën?
2021-03-19 01:51:20,015 - INFO - joeynmt.training - 	Hypothesis: ¡jëduꞌun mëwiin nebyë dios teety të myënaꞌanyën, jëduꞌun ajxy jeꞌe këxyë wiinë wiinë wiinë a maas tsobaat!
2021-03-19 01:51:20,015 - INFO - joeynmt.training - Example #2
2021-03-19 01:51:20,015 - DEBUG - joeynmt.training - 	Raw source:     ['¿cuán@@', 'do', 'fue', 'autoridad', 'ju@@', 'an@@', '?']
2021-03-19 01:51:20,015 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¿@@', 'tëë', 'ja', 'juan', 'tëë', 'ty@@', 'o@@', 'pila@@', 'ty@@', '?']
2021-03-19 01:51:20,015 - INFO - joeynmt.training - 	Source:     ¿cuándo fue autoridad juan?
2021-03-19 01:51:20,015 - INFO - joeynmt.training - 	Reference:  ¿mënaa ja juan tëë tyunkmëëdaty?
2021-03-19 01:51:20,016 - INFO - joeynmt.training - 	Hypothesis: ¿tëë ja juan tëë tyopilaty?
2021-03-19 01:51:20,016 - INFO - joeynmt.training - Example #3
2021-03-19 01:51:20,016 - DEBUG - joeynmt.training - 	Raw source:     ['porque', 'cualquiera', 'que', 'se', 'en@@', 'al@@', 'tec@@', 'e,', 'será', 'hu@@', 'mi@@', 'll@@', 'ado', 'y', 'el', 'que', 'se', 'hu@@', 'mi@@', 'll@@', 'a,', 'será', 'en@@', 'al@@', 'te@@', 'ci@@', 'do']
2021-03-19 01:51:20,016 - DEBUG - joeynmt.training - 	Raw hypothesis: ['ix', 'pën', 'jatyë', 'jëyaꞌay', 'yaabë', 'naax', 'wiimbë', 'jëyaꞌay', 'yꞌuunk,', 'x@@', 'uuꞌ@@', 'xt@@', 'aak@@', 'p', 'jeꞌe', 'a', 'pën', 'jeꞌe', 'duꞌun', 'myë@@', 'yujꞌw@@', 'aꞌan@@', 'aꞌany']
2021-03-19 01:51:20,016 - INFO - joeynmt.training - 	Source:     porque cualquiera que se enaltece, será humillado y el que se humilla, será enaltecido
2021-03-19 01:51:20,016 - INFO - joeynmt.training - 	Reference:  ix pën jaty nëmyëjjaantsypëdaakëp, tsyuuk pëdaꞌaganëp jeꞌe a pën nëtsyuukpëdaakëp, jeꞌejëts mëj jaantsy pëdaꞌaganëp
2021-03-19 01:51:20,016 - INFO - joeynmt.training - 	Hypothesis: ix pën jatyë jëyaꞌay yaabë naax wiimbë jëyaꞌay yꞌuunk, xuuꞌxtaakp jeꞌe a pën jeꞌe duꞌun myëyujꞌwaꞌanaꞌany
2021-03-19 01:51:20,016 - INFO - joeynmt.training - Validation result (greedy) at epoch  70, step     3000: bleu:   5.36, loss: 87824.2969, ppl:  44.6766, duration: 96.0916s
2021-03-19 01:51:36,044 - INFO - joeynmt.training - Epoch  70: total training loss 104.18
2021-03-19 01:51:36,044 - INFO - joeynmt.training - EPOCH 71
2021-03-19 01:52:38,511 - INFO - joeynmt.training - Epoch  71: total training loss 102.86
2021-03-19 01:52:38,511 - INFO - joeynmt.training - EPOCH 72
2021-03-19 01:53:51,072 - INFO - joeynmt.training - Epoch  72: total training loss 102.20
2021-03-19 01:53:51,073 - INFO - joeynmt.training - EPOCH 73
2021-03-19 01:54:59,118 - INFO - joeynmt.training - Epoch  73: total training loss 101.83
2021-03-19 01:54:59,118 - INFO - joeynmt.training - EPOCH 74
2021-03-19 01:56:03,168 - INFO - joeynmt.training - Epoch  74: total training loss 100.14
2021-03-19 01:56:03,169 - INFO - joeynmt.training - EPOCH 75
2021-03-19 01:57:15,386 - INFO - joeynmt.training - Epoch  75: total training loss 99.09
2021-03-19 01:57:15,386 - INFO - joeynmt.training - EPOCH 76
2021-03-19 01:58:20,993 - INFO - joeynmt.training - Epoch  76: total training loss 97.67
2021-03-19 01:58:20,993 - INFO - joeynmt.training - EPOCH 77
2021-03-19 01:59:30,118 - INFO - joeynmt.training - Epoch  77: total training loss 96.87
2021-03-19 01:59:30,118 - INFO - joeynmt.training - EPOCH 78
2021-03-19 02:00:40,722 - INFO - joeynmt.training - Epoch  78: total training loss 95.89
2021-03-19 02:00:40,723 - INFO - joeynmt.training - EPOCH 79
2021-03-19 02:01:50,920 - INFO - joeynmt.training - Epoch  79: total training loss 95.11
2021-03-19 02:01:50,920 - INFO - joeynmt.training - EPOCH 80
2021-03-19 02:02:54,160 - INFO - joeynmt.training - Epoch  80: total training loss 94.04
2021-03-19 02:02:54,160 - INFO - joeynmt.training - EPOCH 81
2021-03-19 02:04:04,732 - INFO - joeynmt.training - Epoch  81: total training loss 93.24
2021-03-19 02:04:04,733 - INFO - joeynmt.training - EPOCH 82
2021-03-19 02:05:48,879 - INFO - joeynmt.training - Example #0
2021-03-19 02:05:48,880 - DEBUG - joeynmt.training - 	Raw source:     ['promo@@', 'ver', 'el', 'uso', 'de', 'las', 'lengu@@', 'as']
2021-03-19 02:05:48,880 - DEBUG - joeynmt.training - 	Raw hypothesis: ['majk', 'pe@@', 'kyë@@', 'xë@@', 'pë']
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Source:     promover el uso de las lenguas
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Reference:  yajtuyo'opy je ajxy jeky aambyë
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Hypothesis: majk pekyëxëpë
2021-03-19 02:05:48,880 - INFO - joeynmt.training - Example #1
2021-03-19 02:05:48,880 - DEBUG - joeynmt.training - 	Raw source:     ['pues', 'si', 'david', 'le', 'lla@@', 'ma', 'señor,', '¿cómo', 'es', 'su', 'hij@@', 'o?']
2021-03-19 02:05:48,880 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¡@@', 'kaꞌa', 'ajxy', 'm@@', 'guꞌu@@', 'kꞌaj@@', 'pëd@@', 'ëj@@', 'k@@', '!']
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Source:     pues si david le llama señor, ¿cómo es su hijo?
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Reference:  ¿nebyëk tyijyë david ja cristo jeꞌe duꞌun yꞌapꞌaty yꞌokꞌaty, pë david ja cristo jëduꞌun amdsoo yajxëꞌajp mëj windsën?
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Hypothesis: ¡kaꞌa ajxy mguꞌukꞌajpëdëjk!
2021-03-19 02:05:48,880 - INFO - joeynmt.training - Example #2
2021-03-19 02:05:48,880 - DEBUG - joeynmt.training - 	Raw source:     ['¿cuán@@', 'do', 'fue', 'autoridad', 'ju@@', 'an@@', '?']
2021-03-19 02:05:48,880 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¿@@', 'tëë', 'ja', 'juan', 'tëë', 'ty@@', 'o@@', 'pila@@', 'ty@@', '?']
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Source:     ¿cuándo fue autoridad juan?
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Reference:  ¿mënaa ja juan tëë tyunkmëëdaty?
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Hypothesis: ¿tëë ja juan tëë tyopilaty?
2021-03-19 02:05:48,880 - INFO - joeynmt.training - Example #3
2021-03-19 02:05:48,880 - DEBUG - joeynmt.training - 	Raw source:     ['porque', 'cualquiera', 'que', 'se', 'en@@', 'al@@', 'tec@@', 'e,', 'será', 'hu@@', 'mi@@', 'll@@', 'ado', 'y', 'el', 'que', 'se', 'hu@@', 'mi@@', 'll@@', 'a,', 'será', 'en@@', 'al@@', 'te@@', 'ci@@', 'do']
2021-03-19 02:05:48,880 - DEBUG - joeynmt.training - 	Raw hypothesis: ['ix', 'pën', 'jatyë', 'jëyaꞌay', 'yaabë', 'naax', 'wiimbë', 'ajxy', 'yujꞌw@@', 'aꞌan@@', 'yëë,', 'a', 'pën', 'jeꞌe', 'duꞌun', 'jyuꞌu@@', 'ky@@', 'ꞌaty', 'nebyë', 'diosë', 'myëdyaꞌaky', 'jëgapxnajxpë', 'ajxy', 'ënajty', 'të', 'tsyo@@', 'kyën,', 'a', 'ku', 'ajxy', 'jeꞌe', 'pyaad@@', 'aꞌanyëë']
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Source:     porque cualquiera que se enaltece, será humillado y el que se humilla, será enaltecido
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Reference:  ix pën jaty nëmyëjjaantsypëdaakëp, tsyuuk pëdaꞌaganëp jeꞌe a pën nëtsyuukpëdaakëp, jeꞌejëts mëj jaantsy pëdaꞌaganëp
2021-03-19 02:05:48,880 - INFO - joeynmt.training - 	Hypothesis: ix pën jatyë jëyaꞌay yaabë naax wiimbë ajxy yujꞌwaꞌanyëë, a pën jeꞌe duꞌun jyuꞌukyꞌaty nebyë diosë myëdyaꞌaky jëgapxnajxpë ajxy ënajty të tsyokyën, a ku ajxy jeꞌe pyaadaꞌanyëë
2021-03-19 02:05:48,881 - INFO - joeynmt.training - Validation result (greedy) at epoch  82, step     3500: bleu:   5.12, loss: 87927.3125, ppl:  44.8762, duration: 79.5233s
2021-03-19 02:06:30,021 - INFO - joeynmt.training - Epoch  82: total training loss 92.42
2021-03-19 02:06:30,022 - INFO - joeynmt.training - EPOCH 83
2021-03-19 02:07:35,477 - INFO - joeynmt.training - Epoch  83: total training loss 91.50
2021-03-19 02:07:35,478 - INFO - joeynmt.training - EPOCH 84
2021-03-19 02:08:38,253 - INFO - joeynmt.training - Epoch  84: total training loss 90.75
2021-03-19 02:08:38,253 - INFO - joeynmt.training - EPOCH 85
2021-03-19 02:09:47,481 - INFO - joeynmt.training - Epoch  85: total training loss 90.28
2021-03-19 02:09:47,481 - INFO - joeynmt.training - EPOCH 86
2021-03-19 02:10:56,886 - INFO - joeynmt.training - Epoch  86: total training loss 89.06
2021-03-19 02:10:56,888 - INFO - joeynmt.training - EPOCH 87
2021-03-19 02:12:04,514 - INFO - joeynmt.training - Epoch  87: total training loss 88.14
2021-03-19 02:12:04,514 - INFO - joeynmt.training - EPOCH 88
2021-03-19 02:13:08,134 - INFO - joeynmt.training - Epoch  88: total training loss 87.42
2021-03-19 02:13:08,136 - INFO - joeynmt.training - EPOCH 89
2021-03-19 02:14:19,880 - INFO - joeynmt.training - Epoch  89: total training loss 87.00
2021-03-19 02:14:19,881 - INFO - joeynmt.training - EPOCH 90
2021-03-19 02:15:29,041 - INFO - joeynmt.training - Epoch  90: total training loss 86.06
2021-03-19 02:15:29,042 - INFO - joeynmt.training - EPOCH 91
2021-03-19 02:16:40,931 - INFO - joeynmt.training - Epoch  91: total training loss 85.20
2021-03-19 02:16:40,931 - INFO - joeynmt.training - EPOCH 92
2021-03-19 02:17:50,360 - INFO - joeynmt.training - Epoch  92: total training loss 84.45
2021-03-19 02:17:50,360 - INFO - joeynmt.training - EPOCH 93
2021-03-19 02:18:58,312 - INFO - joeynmt.training - Epoch  93: total training loss 84.18
2021-03-19 02:18:58,312 - INFO - joeynmt.training - EPOCH 94
2021-03-19 02:19:00,335 - INFO - joeynmt.training - Epoch  94, Step:     4000, Batch Loss:     2.372434, Tokens per Sec:     2071, Lr: 0.000300
2021-03-19 02:20:31,254 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2021-03-19 02:20:31,255 - INFO - joeynmt.training - Saving new checkpoint.
2021-03-19 02:20:31,545 - INFO - joeynmt.training - Example #0
2021-03-19 02:20:31,546 - DEBUG - joeynmt.training - 	Raw source:     ['promo@@', 'ver', 'el', 'uso', 'de', 'las', 'lengu@@', 'as']
2021-03-19 02:20:31,546 - DEBUG - joeynmt.training - 	Raw hypothesis: ['majk', 'ki@@', 'lo', 'jy@@', 'em@@', 'mbë']
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Source:     promover el uso de las lenguas
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Reference:  yajtuyo'opy je ajxy jeky aambyë
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Hypothesis: majk kilo jyemmbë
2021-03-19 02:20:31,546 - INFO - joeynmt.training - Example #1
2021-03-19 02:20:31,546 - DEBUG - joeynmt.training - 	Raw source:     ['pues', 'si', 'david', 'le', 'lla@@', 'ma', 'señor,', '¿cómo', 'es', 'su', 'hij@@', 'o?']
2021-03-19 02:20:31,546 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¡@@', 'kaꞌa', 'ja', 'yaꞌay', 'ajxy', 'm@@', 'ꞌix@@', 'yꞌaj@@', 'pë@@', '!']
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Source:     pues si david le llama señor, ¿cómo es su hijo?
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Reference:  ¿nebyëk tyijyë david ja cristo jeꞌe duꞌun yꞌapꞌaty yꞌokꞌaty, pë david ja cristo jëduꞌun amdsoo yajxëꞌajp mëj windsën?
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Hypothesis: ¡kaꞌa ja yaꞌay ajxy mꞌixyꞌajpë!
2021-03-19 02:20:31,546 - INFO - joeynmt.training - Example #2
2021-03-19 02:20:31,546 - DEBUG - joeynmt.training - 	Raw source:     ['¿cuán@@', 'do', 'fue', 'autoridad', 'ju@@', 'an@@', '?']
2021-03-19 02:20:31,546 - DEBUG - joeynmt.training - 	Raw hypothesis: ['¿@@', 'tëë', 'ja', 'juan', 'tëë', 'ty@@', 'o@@', 'pila@@', 'ty@@', '?']
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Source:     ¿cuándo fue autoridad juan?
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Reference:  ¿mënaa ja juan tëë tyunkmëëdaty?
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Hypothesis: ¿tëë ja juan tëë tyopilaty?
2021-03-19 02:20:31,546 - INFO - joeynmt.training - Example #3
2021-03-19 02:20:31,546 - DEBUG - joeynmt.training - 	Raw source:     ['porque', 'cualquiera', 'que', 'se', 'en@@', 'al@@', 'tec@@', 'e,', 'será', 'hu@@', 'mi@@', 'll@@', 'ado', 'y', 'el', 'que', 'se', 'hu@@', 'mi@@', 'll@@', 'a,', 'será', 'en@@', 'al@@', 'te@@', 'ci@@', 'do']
2021-03-19 02:20:31,546 - DEBUG - joeynmt.training - 	Raw hypothesis: ['ix', 'jëduꞌun', 'mëwiin', 'nebyë', 'diosë', 'yꞌ@@', 'ooꞌ@@', 'x@@', 'pën,', 'ja', 'jëyaꞌay', 'ajxy', 'jeꞌe', 'yaj@@', 'moꞌo@@', 'w@@', 'aambyë', 'kyuꞌu@@', 'dujt', 'jeꞌe', 'gyëxpë', 'ku', 'ajxy', 'ënajty', 'të', 'yꞌi@@', 'xy']
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Source:     porque cualquiera que se enaltece, será humillado y el que se humilla, será enaltecido
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Reference:  ix pën jaty nëmyëjjaantsypëdaakëp, tsyuuk pëdaꞌaganëp jeꞌe a pën nëtsyuukpëdaakëp, jeꞌejëts mëj jaantsy pëdaꞌaganëp
2021-03-19 02:20:31,546 - INFO - joeynmt.training - 	Hypothesis: ix jëduꞌun mëwiin nebyë diosë yꞌooꞌxpën, ja jëyaꞌay ajxy jeꞌe yajmoꞌowaambyë kyuꞌudujt jeꞌe gyëxpë ku ajxy ënajty të yꞌixy
2021-03-19 02:20:31,546 - INFO - joeynmt.training - Validation result (greedy) at epoch  94, step     4000: bleu:   5.79, loss: 87640.2969, ppl:  44.3224, duration: 91.2093s
2021-03-19 02:21:35,267 - INFO - joeynmt.training - Epoch  94: total training loss 82.89
2021-03-19 02:21:35,269 - INFO - joeynmt.training - EPOCH 95
2021-03-19 02:22:45,339 - INFO - joeynmt.training - Epoch  95: total training loss 82.27
2021-03-19 02:22:45,340 - INFO - joeynmt.training - EPOCH 96
2021-03-19 02:23:55,098 - INFO - joeynmt.training - Epoch  96: total training loss 81.60
2021-03-19 02:23:55,098 - INFO - joeynmt.training - EPOCH 97
2021-03-19 02:25:03,600 - INFO - joeynmt.training - Epoch  97: total training loss 80.79
2021-03-19 02:25:03,600 - INFO - joeynmt.training - EPOCH 98
2021-03-19 02:26:19,906 - INFO - joeynmt.training - Epoch  98: total training loss 80.33
2021-03-19 02:26:19,907 - INFO - joeynmt.training - EPOCH 99
2021-03-19 02:27:32,823 - INFO - joeynmt.training - Epoch  99: total training loss 79.87
2021-03-19 02:27:32,826 - INFO - joeynmt.training - EPOCH 100
2021-03-19 02:28:44,745 - INFO - joeynmt.training - Epoch 100: total training loss 79.06
2021-03-19 02:28:44,746 - INFO - joeynmt.training - Training ended after 100 epochs.
2021-03-19 02:28:44,746 - INFO - joeynmt.training - Best validation result (greedy) at step     4000:  44.32 ppl.
2021-03-19 02:28:44,767 - INFO - joeynmt.prediction - Process device: cpu, n_gpu: 0, batch_size per device: 256
2021-03-19 02:28:45,161 - INFO - joeynmt.prediction - Decoding on dev set (/data/joeynmt/mir.esmir/dev.bpe.mir)...
2021-03-19 02:29:56,086 - INFO - joeynmt.prediction -  dev bleu[13a]:   5.83 [Beam search decoding with beam size = 3 and alpha = 1.0]
2021-03-19 02:29:56,088 - INFO - joeynmt.prediction - Translations saved to: models/con-random5/00004000.hyps.dev
2021-03-19 02:29:56,088 - INFO - joeynmt.prediction - Decoding on test set (/data/joeynmt/mir.esmir/test.bpe.mir)...
2021-03-19 02:32:40,979 - INFO - joeynmt.prediction - test bleu[13a]:   5.56 [Beam search decoding with beam size = 3 and alpha = 1.0]
2021-03-19 02:32:40,981 - INFO - joeynmt.prediction - Translations saved to: models/con-random5/00004000.hyps.test
